{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Assignment 4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0FwGK1eWO71",
        "colab_type": "text"
      },
      "source": [
        "**Machine Learning Assignment 4 (CNN, AdaBoost, Bagging)**\n",
        "\n",
        "**Author: Jaemin Lee**\n",
        "\n",
        "**Due: 12/2/2019**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ybak3GshzH1",
        "colab_type": "text"
      },
      "source": [
        "Importing a library that is not in google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-VpP0J7WEtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install pytorch on google colab\n",
        "!pip install -q matplotlib-venn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDkKMmx_hrVK",
        "colab_type": "code",
        "outputId": "78c984b4-2eee-4584-aa0f-b51dff0a9239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "!apt-get -qq install -y libfluidsynth1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package libfluidsynth1:amd64.\n",
            "(Reading database ... 145605 files and directories currently installed.)\n",
            "Preparing to unpack .../libfluidsynth1_1.1.9-1_amd64.deb ...\n",
            "Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYwLG9UwI69L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGXgff9P0IZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# google drive import \n",
        "# setting up google drive connection\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAgvW3pTh8Qs",
        "colab_type": "text"
      },
      "source": [
        "Getting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk_Fn_BVMrF5",
        "colab_type": "code",
        "outputId": "75a9df03-56e6-4b43-fb7b-e910a6eb57ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "df = pd.read_csv('gdrive/My Drive/mnist.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel744</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  pixel0  pixel1  pixel2  ...  pixel780  pixel781  pixel782  pixel783\n",
              "0      1       0       0       0  ...         0         0         0         0\n",
              "1      0       0       0       0  ...         0         0         0         0\n",
              "2      1       0       0       0  ...         0         0         0         0\n",
              "3      4       0       0       0  ...         0         0         0         0\n",
              "4      0       0       0       0  ...         0         0         0         0\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9tN_ua0OOkJ",
        "colab_type": "text"
      },
      "source": [
        "Task 1: Subsampling 100 instances from each label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4o6PBXIM8vW",
        "colab_type": "code",
        "outputId": "02c8c7a4-3a09-424f-e428-396a0b7183fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "# create an empty data frame\n",
        "res = pd.DataFrame([])\n",
        "\n",
        "# run the for loop in the range of 10 \n",
        "for i in range(10):\n",
        "  tempFrame = df.loc[df.label == i, :]               # labels from 0 to 9\n",
        "  frame = tempFrame.sample(n=100, random_state=0)    # extract 100 random samples from each label\n",
        "  res = res.append(pd.DataFrame(frame))              # combine all 100 instances from each label\n",
        "\n",
        "print(res)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      label  pixel0  pixel1  pixel2  ...  pixel780  pixel781  pixel782  pixel783\n",
            "9818      0       0       0       0  ...         0         0         0         0\n",
            "8643      0       0       0       0  ...         0         0         0         0\n",
            "2986      0       0       0       0  ...         0         0         0         0\n",
            "5681      0       0       0       0  ...         0         0         0         0\n",
            "6818      0       0       0       0  ...         0         0         0         0\n",
            "...     ...     ...     ...     ...  ...       ...       ...       ...       ...\n",
            "5118      9       0       0       0  ...         0         0         0         0\n",
            "5845      9       0       0       0  ...         0         0         0         0\n",
            "1128      9       0       0       0  ...         0         0         0         0\n",
            "3883      9       0       0       0  ...         0         0         0         0\n",
            "6499      9       0       0       0  ...         0         0         0         0\n",
            "\n",
            "[1000 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jecVmPBN9v6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# separate X and y variables to split test and train data\n",
        "X = res.iloc[:,1:] # all rows and columns except the labels\n",
        "y = res.iloc[:, 0] # all rows and just the labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r03qg59LOLWP",
        "colab_type": "text"
      },
      "source": [
        "Train and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbi5UlArNLAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state =1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwZFdPUtOx_U",
        "colab_type": "text"
      },
      "source": [
        "Task 2: Using PyTorch to build CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhmyQyQ-h4vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper parameters\n",
        "num_epochs = 5\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTSrPXf4UeHX",
        "colab_type": "text"
      },
      "source": [
        "Getting built-in dataset for CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m979P3TfUctW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06KJATQRUtpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv-r-TdmfDZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convolutional neural network (two convolutional layers)\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc1 = nn.Linear(7*7*32, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.softmax = nn.Softmax()\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.softmax(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtaWwsaJPO1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ConvNet(num_classes).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY4781mbPZSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNJ4Ai8gPnA2",
        "colab_type": "code",
        "outputId": "eb4f9b30-d1f2-44bc-cbae-6deccd7a2f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 1.5745\n",
            "Epoch [1/5], Step [200/600], Loss: 1.5478\n",
            "Epoch [1/5], Step [300/600], Loss: 1.4922\n",
            "Epoch [1/5], Step [400/600], Loss: 1.5089\n",
            "Epoch [1/5], Step [500/600], Loss: 1.4941\n",
            "Epoch [1/5], Step [600/600], Loss: 1.4878\n",
            "Epoch [2/5], Step [100/600], Loss: 1.4877\n",
            "Epoch [2/5], Step [200/600], Loss: 1.4807\n",
            "Epoch [2/5], Step [300/600], Loss: 1.4711\n",
            "Epoch [2/5], Step [400/600], Loss: 1.4738\n",
            "Epoch [2/5], Step [500/600], Loss: 1.5004\n",
            "Epoch [2/5], Step [600/600], Loss: 1.4807\n",
            "Epoch [3/5], Step [100/600], Loss: 1.4865\n",
            "Epoch [3/5], Step [200/600], Loss: 1.4737\n",
            "Epoch [3/5], Step [300/600], Loss: 1.4844\n",
            "Epoch [3/5], Step [400/600], Loss: 1.4714\n",
            "Epoch [3/5], Step [500/600], Loss: 1.4710\n",
            "Epoch [3/5], Step [600/600], Loss: 1.4908\n",
            "Epoch [4/5], Step [100/600], Loss: 1.4690\n",
            "Epoch [4/5], Step [200/600], Loss: 1.4914\n",
            "Epoch [4/5], Step [300/600], Loss: 1.4825\n",
            "Epoch [4/5], Step [400/600], Loss: 1.4711\n",
            "Epoch [4/5], Step [500/600], Loss: 1.4620\n",
            "Epoch [4/5], Step [600/600], Loss: 1.4837\n",
            "Epoch [5/5], Step [100/600], Loss: 1.4697\n",
            "Epoch [5/5], Step [200/600], Loss: 1.4734\n",
            "Epoch [5/5], Step [300/600], Loss: 1.4734\n",
            "Epoch [5/5], Step [400/600], Loss: 1.4871\n",
            "Epoch [5/5], Step [500/600], Loss: 1.4805\n",
            "Epoch [5/5], Step [600/600], Loss: 1.5010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtQNUEtLQKA_",
        "colab_type": "code",
        "outputId": "ad8ccf6b-f325-4ef2-eef8-d4a6bf9d66ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Test the model\n",
        "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 98.68 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z457D-Ntcy5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yltmQ_eUdBOR",
        "colab_type": "text"
      },
      "source": [
        "Task 3: AdaBoost and Bagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p--ZlXHZc3mK",
        "colab_type": "code",
        "outputId": "081738c1-520c-4e3b-8921-ed25dd9ea5ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "adaboost_dtree = AdaBoostClassifier(tree.DecisionTreeClassifier(random_state=1))\n",
        "adaboost_dtree_model = adaboost_dtree.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "adaboost_dtree_pred = adaboost_dtree.predict(X_test)\n",
        "print(\"Accuracy for AdaBoost is with Decision Tree is:\",metrics.accuracy_score(y_test, adaboost_dtree_pred)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for AdaBoost is with Decision Tree is: 60.5 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2Pk8pUseGgW",
        "colab_type": "code",
        "outputId": "37498fd5-a19f-4862-baf4-f55387b65b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "adaboost_logistic = AdaBoostClassifier(LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial'))\n",
        "adaboost_log_model = adaboost_logistic.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "adaboost_log_pred = adaboost_logistic.predict(X_test)\n",
        "print(\"Accuracy for AdaBoost with Logistic Regression is:\",metrics.accuracy_score(y_test, adaboost_log_pred)*100,\"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for AdaBoost with Logistic Regression is: 82.0 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6ZNXw06eezq",
        "colab_type": "code",
        "outputId": "935603ee-728c-454d-aa78-609adc01753c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bagging_dtree = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\n",
        "bagging_dtree_model = bagging_dtree.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "bagging_dtree_pred = bagging_dtree.predict(X_test)\n",
        "print(\"Accuracy for Bagging with Decision Tree is:\",metrics.accuracy_score(y_test, bagging_dtree_pred)*100,\"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for Bagging with Decision Tree is: 79.5 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY_VrcRLkWtc",
        "colab_type": "code",
        "outputId": "684c0386-fe8a-4bc9-cfda-6941767fe712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "bagging_log = BaggingClassifier(LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial'))\n",
        "bagging_log_model = bagging_log.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "bagging_log_pred = bagging_log.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r18gOf0skjFr",
        "colab_type": "code",
        "outputId": "2827fb99-ae17-4ee3-9df6-a33a49be1d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Accuracy for Bagging with Logistic Regression is:\",metrics.accuracy_score(y_test, bagging_log_pred)*100,\"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for Bagging with Logistic Regression is: 83.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6elYU3Hk7UU",
        "colab_type": "text"
      },
      "source": [
        "Task 4: Precision and Recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWF9OUPBkuo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "recall_1 = recall_score(y_test, adaboost_dtree_pred, average = 'macro')*100\n",
        "recall_2 = recall_score(y_test, adaboost_log_pred, average = 'macro')*100\n",
        "recall_3 = recall_score(y_test, bagging_dtree_pred, average = 'macro')*100\n",
        "recall_4 = recall_score(y_test, bagging_log_pred, average = 'macro')*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhOv2c3ilz8x",
        "colab_type": "code",
        "outputId": "606da0c7-aac6-42b0-a14a-b12ce92b7e86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print(\"Recall for Adaboost with Decision Tree: \",recall_1,\"%\")\n",
        "print(\"Recall for Adaboost with Logistic Regression: \",recall_2,\"%\")\n",
        "print(\"Recall for Bagging with Decision Tree: \",recall_3,\"%\")\n",
        "print(\"Recall for Bagging with Logistic Regression: \",recall_4,\"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall for Adaboost with Decision Tree:  59.81556420875307 %\n",
            "Recall for Adaboost with Logistic Regression:  81.81087952295384 %\n",
            "Recall for Adaboost with Decision Tree:  77.47922201637371 %\n",
            "Recall for Adaboost with Logistic Regression:  83.16367481847358 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjoR_iFWmK_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "\n",
        "p_1 = precision_score(y_test, adaboost_dtree_pred, average = 'macro')*100\n",
        "p_2 = precision_score(y_test, adaboost_log_pred, average = 'macro')*100\n",
        "p_3 = precision_score(y_test, bagging_dtree_pred, average = 'macro')*100\n",
        "p_4 = precision_score(y_test, bagging_log_pred, average = 'macro')*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XErq_nRUmaol",
        "colab_type": "code",
        "outputId": "76face2a-de0a-4ffc-c60e-517d25935921",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print(\"Precision for Adaboost with Decision Tree: \", p_1,\"%\")\n",
        "print(\"Precision for Adaboost with Logistic Regression: \",p_2,\"%\")\n",
        "print(\"Precision for Bagging with Decision Tree: \", p_3,\"%\")\n",
        "print(\"Precision for Bagging with Logistic Regression: \", p_4,\"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision for Adaboost with Decision Tree:  60.21801950062821 %\n",
            "Precision for Adaboost with Logistic Regression:  81.30774564870676 %\n",
            "Precision for Bagging with Decision Tree:  79.5959595959596 %\n",
            "Precision for Bagging with Logistic Regression:  82.52597402597402 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}